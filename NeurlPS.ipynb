{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb2hYAF3jH8VMWg1J4STfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyPython/NeurlPS/blob/main/NeurlPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "1iyRHJBtWJWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xNnimB9NWAmi"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -n base -c conda-forge mamba\n",
        "!mamba create -n \"lux-s3\" \"python==3.11\"\n",
        "!git clone https://github.com/Lux-AI-Challenge/Lux-Design-S3/\n",
        "!pip install -e Lux-Design-S3/src"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Run Agent"
      ],
      "metadata": {
        "id": "ata1_QJIoLe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'  # Set the backend explicitly\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Force the backend to Agg\n",
        "import matplotlib.pyplot as plt\n",
        "  # Ensure this is set for non-GUI environments\n",
        "print(matplotlib.get_backend())  # Confirm that the backend is correctly set\n",
        "\n",
        "# Run a match with the correct paths\n",
        "!ls /content/Lux-Design-S3/kits/python/\n",
        "\n",
        "!luxai-s3 /content/Lux-Design-S3/kits/python/main.py /content/Lux-Design-S3/kits/python/main.py --output replay.json"
      ],
      "metadata": {
        "id": "3yvDyLU5wyZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Upload replay.json to this link\n",
        "\n",
        "https://s3vis.lux-ai.org/"
      ],
      "metadata": {
        "id": "WxBmItkTWK9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!eval \"$(mamba shell hook --shell)\"\n",
        "!conda install -n base -c conda-forge mamba\n",
        "!mamba activate lux-s3"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9psfLEsAeSNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gymnax Setup"
      ],
      "metadata": {
        "id": "HZJ3_xP69qnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/RobertTLange/gymnax.git@main\n",
        "import gymnax\n",
        "import jax\n",
        "\n",
        "# Check if gymnax is installed and can be imported without errors\n",
        "print(f\"Gymnax installed: {gymnax is not None}\")\n",
        "\n",
        "# Try to access a gymnax environment to further verify installation\n",
        "try:\n",
        "  env, env_params = gymnax.make(\"Catch-bsuite\")\n",
        "  print(f\"Environment 'Catch-bsuite' created successfully\")\n",
        "except Exception as e:\n",
        "  print(f\"Error creating environment: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5g31RLoo9sHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Development Environment"
      ],
      "metadata": {
        "id": "nWpL73NjWZR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lux-AI-Challenge/Lux-Design-S2.git\n",
        "!ls /content/Lux-Design-S3\n",
        "!git fetch --all\n",
        "!git pull origin main\n",
        "%cd /content/Lux-Design-S3/"
      ],
      "metadata": {
        "id": "cGxDr4_dWZlv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Environment"
      ],
      "metadata": {
        "id": "dplFlCZzWk_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the project root directory\n",
        "%cd /content/Lux-Design-S3/src\n",
        "\n",
        "# Install in editable mode\n",
        "!pip install -e .\n",
        "# Change directory to your training area\n",
        "%cd /content/Lux-Design-S3/src/luxai_s3/\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))  # This adds src to sys.path\n",
        "\n",
        "# Restart the kernel to ensure changes take effect. (You might need to run this manually)\n",
        "try:\n",
        "    import luxai_s3\n",
        "except ModuleNotFoundError:\n",
        "    !pip install -e Lux-Design-S3/src\n",
        "    import luxai_s3  # Import after installation\n",
        "# Import necessary modules\n",
        "from env import LuxAIS3Env  # Import after installation\n",
        "from params import EnvParams\n",
        "import importlib\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure agent.py is in the correct path\n",
        "# Updated agent_path to the correct location\n",
        "agent_path = os.path.join(os.getcwd(), \"..\", \"..\", \"kits\", \"python\", \"agent.py\")\n",
        "\n",
        "# If agent.py is not in the current directory, adjust this:\n",
        "# agent_path = \"/path/to/your/agent.py\"\n",
        "\n",
        "# Add the directory containing agent.py to sys.path\n",
        "agent_dir = os.path.dirname(agent_path)\n",
        "sys.path.append(agent_dir)\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))  # This adds Lux-Design-S3 to sys.path\n",
        "\n",
        "# Construct the absolute path to your module\n",
        "module_path = os.path.abspath(os.path.join(os.getcwd(), \"content/Lux-Design-S3/kits/python\"))\n",
        "\n",
        "# Add the path to sys.path if it's not already there\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "# Import the module\n",
        "agent_module = importlib.import_module(\"agent\")  # Assuming 'agent.py' is the module\n",
        "\n",
        "# Access the Agent class from the imported module\n",
        "Agent = agent_module.Agent\n",
        "# Create the LuxAI_S3 environment:\n",
        "env = LuxAIS3Env(fixed_env_params=EnvParams())\n",
        "\n",
        "# Initialize your agents:\n",
        "agents = {\n",
        "    \"player_0\": Agent(player=\"player_0\", env_cfg=env.fixed_env_params),\n",
        "    \"player_1\": Agent(player=\"player_1\", env_cfg=env.fixed_env_params),\n",
        "}"
      ],
      "metadata": {
        "id": "5VMMcuNTvT9v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Based Off-Policy Learning"
      ],
      "metadata": {
        "id": "mHL40ANaWplJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.random\n",
        "from stable_baselines3 import PPO\n",
        "import torch as th\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import torch.nn as nn\n",
        "\n",
        "# Change to the project root directory\n",
        "%cd /content/Lux-Design-S3/src\n",
        "\n",
        "# Install in editable mode\n",
        "!pip install -e .\n",
        "\n",
        "# Change directory to your training area\n",
        "%cd /content/Lux-Design-S3/src/luxai_s3/\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))  # This adds src to sys.path\n",
        "\n",
        "# Restart the kernel to ensure changes take effect. (You might need to run this manually)\n",
        "try:\n",
        "    import luxai_s3\n",
        "except ModuleNotFoundError:\n",
        "    !pip install -e /content/Lux-Design-S3/src\n",
        "    import luxai_s3  # Import after installation\n",
        "\n",
        "# Import necessary modules\n",
        "from luxai_s3.spaces import MultiDiscrete  # Import MultiDiscrete\n",
        "from luxai_s3.env import LuxAIS3Env\n",
        "from luxai_s3.params import EnvParams\n",
        "from luxai_s3 import utils\n",
        "\n",
        "# Create the LuxAI_S3 environment:\n",
        "env = LuxAIS3Env(fixed_env_params=EnvParams())\n",
        "\n",
        "def flatten_observation(obs):\n",
        "    # Check if the observation is a numpy ndarray\n",
        "    if isinstance(obs, np.ndarray):\n",
        "        return obs.flatten()\n",
        "    # If the observation is of another type, you could handle it here (if necessary)\n",
        "    else:\n",
        "        raise ValueError(\"Expected a numpy.ndarray for observation\")\n",
        "\n",
        "# Check the action space directly (using method call)\n",
        "original_action_space = env.action_space()\n",
        "print(\"original_action_space:\", original_action_space)\n",
        "print(\"Type of original_action_space:\", type(original_action_space))\n",
        "print(f\"Type of observation: {type(obs)}\")\n",
        "print(f\"Shape of observation: {getattr(obs, 'shape', 'No shape attribute')}\")\n",
        "\n",
        "# Define the custom wrapper\n",
        "class LuxAIWrapper(gym.Env):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "\n",
        "        # Get the original action space\n",
        "        original_action_space = env.action_space()\n",
        "\n",
        "        # Handle action space if it's a Dict\n",
        "        if isinstance(original_action_space, gym.spaces.Dict):\n",
        "            # Print out the individual spaces in the Dict\n",
        "            for key, space in original_action_space.spaces.items():\n",
        "                print(f\"Action space for {key}: {space}\")\n",
        "            self.action_space = original_action_space\n",
        "        else:\n",
        "            print(\"original_action_space is not a Dict, it's of type:\", type(original_action_space))\n",
        "            self.action_space = original_action_space  # Or use another appropriate default\n",
        "\n",
        "        # Get the raw observation and reshape it\n",
        "        sample_obs, _ = env.reset(jax.random.PRNGKey(0))\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = sample_obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Check the raw observation shape before reshaping\n",
        "        print(f\"Raw observation shape for player_0: {player_obs_np.shape}\")\n",
        "\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        reshaped_obs = reshape_observation(player_obs_np)\n",
        "\n",
        "        if reshaped_obs is None:\n",
        "            raise ValueError(\"Observation reshaping failed. Please check your reshaping logic.\")\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=reshaped_obs.shape[1:],  # Exclude batch dimension\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        key = jax.random.PRNGKey(seed if seed else 0)\n",
        "        obs, info = self.env.reset(key)\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Flatten and reshape the observation as necessary\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        player_obs_np = reshape_observation(player_obs_np)\n",
        "\n",
        "        return player_obs_np, info\n",
        "\n",
        "    def step(self, action):\n",
        "        lux_action = self.convert_action(action)\n",
        "        obs, reward, done, truncated, info = self.env.step(lux_action)\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Flatten and reshape the observation as necessary\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        player_obs_np = reshape_observation(player_obs_np)\n",
        "\n",
        "        return player_obs_np, reward, done, truncated, info\n",
        "\n",
        "    def convert_action(self, action):\n",
        "        \"\"\"\n",
        "        Converts a Dict action to your LuxAIS3Env's action space.\n",
        "        \"\"\"\n",
        "        # Convert the action for both players (or any other components). Adjust as needed.\n",
        "        return {\"player_0\": action[\"player_0\"].tolist(), \"player_1\": action[\"player_1\"].tolist()}\n",
        "\n",
        "# Create the environment function for training\n",
        "def make_env():\n",
        "    env = LuxAIS3Env(fixed_env_params=EnvParams())\n",
        "    return LuxAIWrapper(env)\n",
        "\n",
        "# Use the wrapper to create vectorized environments for training\n",
        "vec_env = DummyVecEnv([make_env])\n",
        "\n",
        "# Custom CNN for feature extraction\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            sample_obs = th.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(sample_obs).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations.float()))\n",
        "\n",
        "# Define PPO model with CustomCNN for feature extraction\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    policy_kwargs=dict(\n",
        "        features_extractor_class=CustomCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=128),\n",
        "        net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Check the action space directly (using method call)\n",
        "original_action_space = env.action_space()\n",
        "print(\"original_action_space:\", original_action_space)\n",
        "print(\"Type of original_action_space:\", type(original_action_space))\n",
        "\n",
        "# Define the custom wrapper\n",
        "class LuxAIWrapper(gym.Env):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "\n",
        "        # Get the original action space\n",
        "        original_action_space = env.action_space()\n",
        "\n",
        "        # Handle action space if it's a Dict\n",
        "        if isinstance(original_action_space, gym.spaces.Dict):\n",
        "            # Print out the individual spaces in the Dict\n",
        "            for key, space in original_action_space.spaces.items():\n",
        "                print(f\"Action space for {key}: {space}\")\n",
        "            self.action_space = original_action_space\n",
        "        else:\n",
        "            print(\"original_action_space is not a Dict, it's of type:\", type(original_action_space))\n",
        "            self.action_space = original_action_space  # Or use another appropriate default\n",
        "\n",
        "        # Get the raw observation and reshape it\n",
        "        sample_obs, _ = env.reset(jax.random.PRNGKey(0))\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = sample_obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Check the raw observation shape before reshaping\n",
        "        print(f\"Raw observation shape for player_0: {player_obs_np.shape}\")\n",
        "\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        reshaped_obs = reshape_observation(player_obs_np)\n",
        "\n",
        "        if reshaped_obs is None:\n",
        "            raise ValueError(\"Observation reshaping failed. Please check your reshaping logic.\")\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=reshaped_obs.shape[1:],  # Exclude batch dimension\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        key = jax.random.PRNGKey(seed if seed is not None else 0)\n",
        "        obs, info = self.env.reset(key)\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Flatten and reshape the observation as necessary\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        player_obs_np = reshape_observation(player_obs_np)\n",
        "\n",
        "        return player_obs_np, info\n",
        "\n",
        "    def step(self, action):\n",
        "        lux_action = self.convert_action(action)\n",
        "        obs, reward, done, truncated, info = self.env.step(lux_action)\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Flatten and reshape the observation as necessary\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        player_obs_np = reshape_observation(player_obs_np)\n",
        "\n",
        "        return player_obs_np, reward, done, truncated, info\n",
        "\n",
        "    def convert_action(self, action):\n",
        "        \"\"\"\n",
        "        Converts a Dict action to your LuxAIS3Env's action space.\n",
        "        \"\"\"\n",
        "        # Convert the action for both players (or any other components). Adjust as needed.\n",
        "        return {\"player_0\": action[\"player_0\"].tolist(), \"player_1\": action[\"player_1\"].tolist()}\n",
        "\n",
        "# Create the environment function for training\n",
        "def make_env():\n",
        "    env = LuxAIS3Env(fixed_env_params=EnvParams())\n",
        "    return LuxAIWrapper(env)\n",
        "\n",
        "# Use the wrapper to create vectorized environments for training\n",
        "vec_env = DummyVecEnv([make_env])\n",
        "\n",
        "# Custom CNN for feature extraction\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            sample_obs = th.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(sample_obs).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations.float()))\n",
        "\n",
        "# Define PPO model with CustomCNN for feature extraction\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    policy_kwargs=dict(\n",
        "        features_extractor_class=CustomCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=128),\n",
        "        net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Change directory to your training area\n",
        "%cd /content/Lux-Design-S3/src/luxai_s3/\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))  # This adds src to sys.path\n",
        "\n",
        "# Restart the kernel to ensure changes take effect. (You might need to run this manually)\n",
        "try:\n",
        "    import luxai_s3\n",
        "except ModuleNotFoundError:\n",
        "    !pip install -e /content/Lux-Design-S3/src\n",
        "    import luxai_s3  # Import after installation\n",
        "\n",
        "# Check the action space directly (using method call)\n",
        "original_action_space = env.action_space()\n",
        "print(\"original_action_space:\", original_action_space)\n",
        "print(\"Type of original_action_space:\", type(original_action_space))\n",
        "\n",
        "# Define the custom wrapper\n",
        "class LuxAIWrapper(gym.Env):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "\n",
        "        # Get the original action space\n",
        "        original_action_space = env.action_space()\n",
        "\n",
        "        # Handle action space if it's a Dict\n",
        "        if isinstance(original_action_space, gym.spaces.Dict):\n",
        "            # Print out the individual spaces in the Dict\n",
        "            for key, space in original_action_space.spaces.items():\n",
        "                print(f\"Action space for {key}: {space}\")\n",
        "            self.action_space = original_action_space\n",
        "        else:\n",
        "            print(\"original_action_space is not a Dict, it's of type:\", type(original_action_space))\n",
        "            self.action_space = original_action_space  # Or use another appropriate default\n",
        "\n",
        "        # Get the raw observation and reshape it\n",
        "        sample_obs, _ = env.reset(jax.random.PRNGKey(0))\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = sample_obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Check the raw observation shape before reshaping\n",
        "        print(f\"Raw observation shape for player_0: {player_obs_np.shape}\")\n",
        "\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        reshaped_obs = reshape_observation(player_obs_np)\n",
        "\n",
        "        if reshaped_obs is None:\n",
        "            raise ValueError(\"Observation reshaping failed. Please check your reshaping logic.\")\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=reshaped_obs.shape[1:],  # Exclude batch dimension\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        key = jax.random.PRNGKey(seed if seed is not None else 0)\n",
        "        obs, info = self.env.reset(key)\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Flatten and reshape the observation as necessary\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        player_obs_np = reshape_observation(player_obs_np)\n",
        "\n",
        "        return player_obs_np, info\n",
        "\n",
        "    def step(self, action):\n",
        "        lux_action = self.convert_action(action)\n",
        "        obs, reward, done, truncated, info = self.env.step(lux_action)\n",
        "\n",
        "        # Extract observation for 'player_0' (or 'player_1')\n",
        "        player_obs = obs.get('player_0', None)  # Adjust key for player_1 if necessary\n",
        "        if player_obs is None:\n",
        "            raise ValueError(\"Failed to extract player observation from the dictionary.\")\n",
        "\n",
        "        # Convert the observation to numpy and flatten it\n",
        "        player_obs_np = utils.to_numpy(player_obs)\n",
        "\n",
        "        # Flatten and reshape the observation as necessary\n",
        "        player_obs_np = flatten_observation(player_obs_np)\n",
        "        player_obs_np = reshape_observation(player_obs_np)\n",
        "\n",
        "        return player_obs_np, reward, done, truncated, info\n",
        "\n",
        "    def convert_action(self, action):\n",
        "        \"\"\"\n",
        "        Converts a Dict action to your LuxAIS3Env's action space.\n",
        "        \"\"\"\n",
        "        # Convert the action for both players (or any other components). Adjust as needed.\n",
        "        return {\"player_0\": action[\"player_0\"].tolist(), \"player_1\": action[\"player_1\"].tolist()}\n",
        "\n",
        "# Create the environment function for training\n",
        "def make_env():\n",
        "    env = LuxAIS3Env(fixed_env_params=EnvParams())\n",
        "    return LuxAIWrapper(env)\n",
        "\n",
        "# Use the wrapper to create vectorized environments for training\n",
        "vec_env = DummyVecEnv([make_env])\n",
        "\n",
        "# Custom CNN for feature extraction\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute the size of the output after the convolutions\n",
        "        with th.no_grad():\n",
        "            sample_obs = th.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(sample_obs).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations.float()))\n",
        "\n",
        "\n",
        "\n",
        "# Define PPO model with CustomCNN for feature extraction\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    policy_kwargs=dict(\n",
        "        features_extractor_class=CustomCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=128),\n",
        "        net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Clip the gradients during training to prevent exploding gradients:\n",
        "model.policy.optimizer = th.optim.Adam(model.policy.parameters(), lr=3e-4)  # Replace with your learning rate\n",
        "model.policy.optimizer.clip_grad_norm_ = 1.0  # Clip gradients with norm greater than 1.0\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10000)  # Adjust total timesteps as needed.\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"ppo_luxai_model\")"
      ],
      "metadata": {
        "id": "sTN1ETd4Wp5v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running The Model In A Game"
      ],
      "metadata": {
        "id": "n-vU-7vWWv9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from luxai_s3.env import LuxAI_S3\n",
        "\n",
        "env = LuxAI_S2()\n",
        "obs = env.reset()\n",
        "\n",
        "for step in range(100):\n",
        "    action = model.predict(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        obs = env.reset()"
      ],
      "metadata": {
        "id": "PShhhtTYWwNa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit Trained Model"
      ],
      "metadata": {
        "id": "tVCOuIZaW2Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle competitions submit -c lux-ai-season-3 -f submission.py -m \"PPO model for Lux AI\"\n",
        "tar -czvf submission.tar.gz *"
      ],
      "metadata": {
        "id": "Fz_igkIZW2Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Shared Experience Buffer"
      ],
      "metadata": {
        "id": "K7qSVZBoW7kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class SharedExperienceBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        # Buffer stores (state, action, reward, next_state)\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def store(self, experience):\n",
        "        \"\"\"Store experience in the buffer.\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Sample a batch of experiences from the buffer.\"\"\"\n",
        "        indices = np.random.choice(len(self.buffer), self.batch_size, replace=False)\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "        return batch\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"Return the current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "AT7L-2bSW79f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate Shared Experience Buffer With Multiple Agents"
      ],
      "metadata": {
        "id": "rb9V5JGcXAUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, player: str, env_cfg, shared_buffer):\n",
        "        self.player = player\n",
        "        self.env_cfg = env_cfg\n",
        "        self.shared_buffer = shared_buffer\n",
        "        self.epsilon = 0.1  # Exploration parameter for epsilon-greedy\n",
        "\n",
        "    def act(self, obs):\n",
        "        # Sample action based on policy\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(0, self.env_cfg[\"action_space_size\"])  # Random action\n",
        "        else:\n",
        "            return self.policy(obs)  # Determine action using learned policy\n",
        "\n",
        "    def train(self):\n",
        "        # Sample experiences from the shared buffer\n",
        "        batch = self.shared_buffer.sample()\n",
        "\n",
        "        # Train using the batch (e.g., Q-learning, PPO, etc.)\n",
        "        for experience in batch:\n",
        "            state, action, reward, next_state = experience\n",
        "            # Apply update rule based on chosen RL algorithm (e.g., Q-learning, PPO, etc.)\n",
        "            self.update_policy(state, action, reward, next_state)\n",
        "\n",
        "    def update_policy(self, state, action, reward, next_state):\n",
        "        # This function should be implemented with the RL update rules (e.g., Q-update, advantage updates)\n",
        "        pass"
      ],
      "metadata": {
        "id": "kpaAw8XfXAmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Agent Training Process"
      ],
      "metadata": {
        "id": "hOrkKzkuXGea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a shared experience buffer\n",
        "shared_buffer = SharedExperienceBuffer(buffer_size=100000, batch_size=64)\n",
        "\n",
        "# Define a placeholder for env_cfg (you need to replace this with your actual configuration)\n",
        "# Example configuration:\n",
        "env_cfg = {\n",
        "    \"action_space_size\": 5  # Replace with the size of your action space\n",
        "}\n",
        "\n",
        "# Initialize agents\n",
        "agents = [Agent(player=\"player_0\", env_cfg=env_cfg, shared_buffer=shared_buffer),\n",
        "          Agent(player=\"player_1\", env_cfg=env_cfg, shared_buffer=shared_buffer)]\n",
        "\n",
        "# Main training loop\n",
        "for episode in range(num_episodes):\n",
        "    for agent in agents:\n",
        "        # Agents interact with the environment and store their experiences in the shared buffer\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store the experience in the shared buffer\n",
        "            shared_buffer.store((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "\n",
        "        # Each agent trains on experiences from the shared buffer\n",
        "        agent.train()"
      ],
      "metadata": {
        "id": "-5RqyAFkXGxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define The Manager (Strategic Policy)"
      ],
      "metadata": {
        "id": "p1_O7SOVXMDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Manager:\n",
        "    def __init__(self, env_cfg):\n",
        "        self.env_cfg = env_cfg\n",
        "        # Define high-level strategies (e.g., prioritize resources or offense)\n",
        "\n",
        "    def act(self, global_state):\n",
        "        # Decide on a high-level strategy (strategic goal)\n",
        "        strategy = self.select_strategy(global_state)\n",
        "        return strategy\n",
        "\n",
        "    def select_strategy(self, global_state):\n",
        "        # Example: decide whether to focus on defense, offense, or resource gathering\n",
        "        if global_state['resources'] < 50:\n",
        "            return \"gather_resources\"\n",
        "        else:\n",
        "            return \"expand_city\""
      ],
      "metadata": {
        "id": "B3UhLu-cXMUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define The Worker (Tactical Policy)"
      ],
      "metadata": {
        "id": "ncxyEM50XRgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Worker:\n",
        "    def __init__(self, player, env_cfg, manager):\n",
        "        self.player = player\n",
        "        self.env_cfg = env_cfg\n",
        "        self.manager = manager\n",
        "        self.epsilon = 0.1  # Exploration parameter\n",
        "\n",
        "    def act(self, current_state):\n",
        "        # The worker gets the strategy from the manager\n",
        "        strategy = self.manager.act(current_state)\n",
        "\n",
        "        # Depending on the strategy, decide on the tactical actions\n",
        "        if strategy == \"gather_resources\":\n",
        "            return self.gather_resources(current_state)\n",
        "        elif strategy == \"expand_city\":\n",
        "            return self.expand_city(current_state)\n",
        "        else:\n",
        "            return self.defend(current_state)\n",
        "\n",
        "    def gather_resources(self, state):\n",
        "        # Implement the logic for gathering resources tactically\n",
        "        pass\n",
        "\n",
        "    def expand_city(self, state):\n",
        "        # Implement the logic for expanding a city tactically\n",
        "        pass\n",
        "\n",
        "    def defend(self, state):\n",
        "        # Implement the logic for defense tactics\n",
        "        pass"
      ],
      "metadata": {
        "id": "Bms5WEMRXRwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with Hierarchical Reinforcement Learning (HRL)"
      ],
      "metadata": {
        "id": "lHmHyNGQXaJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HRLAgent:\n",
        "    def __init__(self, player, env_cfg):\n",
        "        self.manager = Manager(env_cfg)\n",
        "        self.worker = Worker(player, env_cfg, self.manager)\n",
        "\n",
        "    def train(self, environment):\n",
        "        for episode in range(num_episodes):\n",
        "            state = environment.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                # The worker makes tactical decisions based on the manager's strategy\n",
        "                action = self.worker.act(state)\n",
        "\n",
        "                # Simulate the action and get the reward\n",
        "                next_state, reward, done, _ = environment.step(action)\n",
        "\n",
        "                # Update both the manager and the worker based on the reward\n",
        "                self.manager.update(state, reward, next_state)\n",
        "                self.worker.update(state, reward, next_state)"
      ],
      "metadata": {
        "id": "EC65cLNfXVOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refine RL Policy Actions with Monte Carlo Tree Search (MCTS)"
      ],
      "metadata": {
        "id": "8u2fXiIBXw_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class MCTSNode:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state  # Game state at this node\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0\n",
        "\n",
        "    def select_best_child(self):\n",
        "        return max(self.children, key=lambda child: child.value / (child.visits + 1e-6))  # Avoid division by zero\n",
        "\n",
        "    def expand(self, action_space):\n",
        "        for action in action_space:\n",
        "            new_state = self.simulate_action(self.state, action)\n",
        "            self.children.append(MCTSNode(new_state, parent=self))\n",
        "\n",
        "    def simulate_action(self, state, action):\n",
        "        \"\"\" Simulates a move and returns a new state \"\"\"\n",
        "        new_state = state.copy()\n",
        "        new_state[\"action_taken\"] = action\n",
        "        return new_state\n",
        "\n",
        "    def backpropagate(self, reward):\n",
        "        node = self\n",
        "        while node:\n",
        "            node.visits += 1\n",
        "            node.value += reward\n",
        "            node = node.parent\n",
        "\n",
        "def mcts_search(root_state, action_space, iterations=100):\n",
        "    root = MCTSNode(root_state)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        node = root\n",
        "\n",
        "        # Selection\n",
        "        while node.children:\n",
        "            node = node.select_best_child()\n",
        "\n",
        "        # Expansion\n",
        "        if not node.children:\n",
        "            node.expand(action_space)\n",
        "\n",
        "        # Simulation\n",
        "        simulated_reward = random.uniform(0, 1)  # Replace with real reward function\n",
        "        node.backpropagate(simulated_reward)\n",
        "\n",
        "    return root.select_best_child().state[\"action_taken\"]  # Best move found"
      ],
      "metadata": {
        "id": "lqcSH_FCXxO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Policy Network"
      ],
      "metadata": {
        "id": "kCiDHHC0X5w9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNNPolicy(nn.Module):\n",
        "    def __init__(self, map_size, action_dim):\n",
        "        super(CNNPolicy, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, padding=1)  # 4 input channels (state)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * map_size * map_size, 128)\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(x.shape[0], -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.softmax(self.fc2(x), dim=-1)  # Output action probabilities"
      ],
      "metadata": {
        "id": "y3kOlWysX6Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Convolutional Networks"
      ],
      "metadata": {
        "id": "jnAbPorzX9D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from dgl.nn import GraphConv\n",
        "\n",
        "class GNNPolicy(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
        "        super(GNNPolicy, self).__init__()\n",
        "        self.conv1 = GraphConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GraphConv(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        x = F.relu(self.conv1(g, features))\n",
        "        x = self.conv2(g, x)\n",
        "        return F.softmax(x, dim=-1)  # Action probabilities"
      ],
      "metadata": {
        "id": "lHkonLECX9R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN & GNN Representation"
      ],
      "metadata": {
        "id": "6JEaTrpEYCAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import dgl\n",
        "from dgl.nn import GraphConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def encode_state_cnn(obs, team_id, env_cfg):\n",
        "    \"\"\"Convert game state into a multi-channel image representation for CNN input\"\"\"\n",
        "    map_size = env_cfg[\"map_width\"]  # Assuming square grid\n",
        "\n",
        "    state_tensor = np.zeros((4, map_size, map_size), dtype=np.float32)  # 4 channels\n",
        "\n",
        "    # Channel 1: Unit positions\n",
        "    unit_positions = np.array(obs[\"units\"][\"position\"][team_id])\n",
        "    for pos in unit_positions:\n",
        "        x, y = pos\n",
        "        state_tensor[0, x, y] = 1  # Player’s unit\n",
        "\n",
        "    opponent_team_id = 1 - team_id\n",
        "    opponent_positions = np.array(obs[\"units\"][\"position\"][opponent_team_id])\n",
        "    for pos in opponent_positions:\n",
        "        x, y = pos\n",
        "        state_tensor[0, x, y] = -1  # Opponent’s unit\n",
        "\n",
        "    # Channel 2: Energy levels (normalized)\n",
        "    unit_energys = np.array(obs[\"units\"][\"energy\"][team_id])\n",
        "    for i, pos in enumerate(unit_positions):\n",
        "        x, y = pos\n",
        "        state_tensor[1, x, y] = unit_energys[i] / 100.0  # Normalize (assuming max energy = 100)\n",
        "\n",
        "    # Channel 3: Relic locations\n",
        "    relic_positions = np.array(obs[\"relic_nodes\"])\n",
        "    for pos in relic_positions:\n",
        "        x, y = pos\n",
        "        state_tensor[2, x, y] = 1  # Mark relic nodes\n",
        "\n",
        "    # Channel 4: Obstacles (if any)\n",
        "    if \"walls\" in obs:\n",
        "        wall_positions = np.array(obs[\"walls\"])\n",
        "        for pos in wall_positions:\n",
        "            x, y = pos\n",
        "            state_tensor[3, x, y] = 1  # Mark walls\n",
        "\n",
        "    return torch.tensor(state_tensor, dtype=torch.float32)\n",
        "\n",
        "def encode_state_gnn(obs, team_id):\n",
        "    \"\"\"Convert game state into a graph representation for GNN input\"\"\"\n",
        "    g = dgl.DGLGraph()\n",
        "\n",
        "    # Add unit nodes\n",
        "    unit_positions = np.array(obs[\"units\"][\"position\"][team_id])\n",
        "    unit_energys = np.array(obs[\"units\"][\"energy\"][team_id])\n",
        "\n",
        "    num_units = len(unit_positions)\n",
        "    g.add_nodes(num_units)\n",
        "\n",
        "    # Set node features (positions + energy)\n",
        "    pos_features = torch.tensor(unit_positions, dtype=torch.float32)\n",
        "    energy_features = torch.tensor(unit_energys, dtype=torch.float32)\n",
        "    g.ndata[\"pos\"] = pos_features\n",
        "    g.ndata[\"energy\"] = energy_features\n",
        "\n",
        "    # Add edges based on proximity (e.g., within 3 tiles)\n",
        "    for i in range(num_units):\n",
        "        for j in range(i + 1, num_units):\n",
        "            if np.linalg.norm(unit_positions[i] - unit_positions[j]) <= 3:\n",
        "                g.add_edges(i, j)\n",
        "                g.add_edges(j, i)  # Bi-directional edges\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "QLTzLXdiYCO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid CNN & GNN Model"
      ],
      "metadata": {
        "id": "hUDjqzsPYHZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridPolicy(nn.Module):\n",
        "    def __init__(self, map_size, action_dim, hidden_dim=128):\n",
        "        super(HybridPolicy, self).__init__()\n",
        "\n",
        "        # CNN for spatial processing\n",
        "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, padding=1)  # 4 input channels (state)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * map_size * map_size, hidden_dim)\n",
        "\n",
        "        # GNN for relational reasoning\n",
        "        self.gnn1 = GraphConv(3, hidden_dim)  # Input: (x, y, energy)\n",
        "        self.gnn2 = GraphConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion_fc = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "\n",
        "        # Output layer\n",
        "        self.action_fc = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, cnn_state, g, g_features):\n",
        "        # CNN processing\n",
        "        x_cnn = F.relu(self.conv1(cnn_state))\n",
        "        x_cnn = F.relu(self.conv2(x_cnn))\n",
        "        x_cnn = x_cnn.view(x_cnn.shape[0], -1)  # Flatten\n",
        "        x_cnn = F.relu(self.fc1(x_cnn))\n",
        "\n",
        "        # GNN processing\n",
        "        x_gnn = F.relu(self.gnn1(g, g_features))\n",
        "        x_gnn = self.gnn2(g, x_gnn)\n",
        "        x_gnn = x_gnn.mean(dim=0)  # Aggregate node information\n",
        "\n",
        "        # Fusion\n",
        "        x_fused = torch.cat([x_cnn, x_gnn], dim=-1)\n",
        "        x_fused = F.relu(self.fusion_fc(x_fused))\n",
        "\n",
        "        # Output action probabilities\n",
        "        return F.softmax(self.action_fc(x_fused), dim=-1)"
      ],
      "metadata": {
        "id": "johFbqi4YHrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Action Selection"
      ],
      "metadata": {
        "id": "16DYq-T7YMOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def select_action(model, obs, team_id, env_cfg):\n",
        "    cnn_state = encode_state_cnn(obs, team_id, env_cfg).unsqueeze(0)  # Add batch dim\n",
        "    g = encode_state_gnn(obs, team_id)\n",
        "    g_features = g.ndata[\"pos\"]  # Use position as node features\n",
        "\n",
        "    with torch.no_grad():\n",
        "        action_probs = model(cnn_state, g, g_features)\n",
        "\n",
        "    # Choose action stochastically\n",
        "    action = torch.multinomial(action_probs, 1).item()\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "VdTzPx43YMcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Observations"
      ],
      "metadata": {
        "id": "LMVx1fIDYQLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def encode_unit_view(obs, unit_pos, env_cfg):\n",
        "    \"\"\"Encodes a unit’s local view as a CNN tensor.\"\"\"\n",
        "    view_size = 5  # 5x5 grid around the unit\n",
        "    state_tensor = np.zeros((4, view_size, view_size), dtype=np.float32)  # 4 channels\n",
        "\n",
        "    # Get relative positions in local 5x5 grid\n",
        "    x_min, x_max = max(0, unit_pos[0] - 2), min(env_cfg[\"map_width\"], unit_pos[0] + 3)\n",
        "    y_min, y_max = max(0, unit_pos[1] - 2), min(env_cfg[\"map_height\"], unit_pos[1] + 3)\n",
        "\n",
        "    # Extract local region\n",
        "    state_tensor[:, :x_max - x_min, :y_max - y_min] = obs[\"global_map\"][:, x_min:x_max, y_min:y_max]\n",
        "\n",
        "    return torch.tensor(state_tensor, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "3RHxxxJ2YQY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Team Awareness (GNN)"
      ],
      "metadata": {
        "id": "lODVCXO9YU1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "def encode_team_graph(obs, team_id):\n",
        "    \"\"\"Builds a graph where units are connected if they are within 3 tiles.\"\"\"\n",
        "    g = dgl.DGLGraph()\n",
        "\n",
        "    # Get unit positions\n",
        "    unit_positions = np.array(obs[\"units\"][\"position\"][team_id])\n",
        "    num_units = len(unit_positions)\n",
        "    g.add_nodes(num_units)\n",
        "\n",
        "    # Add edges (if within 3 tiles)\n",
        "    for i in range(num_units):\n",
        "        for j in range(i + 1, num_units):\n",
        "            if np.linalg.norm(unit_positions[i] - unit_positions[j]) <= 3:\n",
        "                g.add_edges(i, j)\n",
        "                g.add_edges(j, i)  # Bi-directional\n",
        "\n",
        "    # Node features: (x, y, energy)\n",
        "    unit_energys = np.array(obs[\"units\"][\"energy\"][team_id])\n",
        "    node_features = torch.tensor(np.hstack([unit_positions, unit_energys]), dtype=torch.float32)\n",
        "\n",
        "    g.ndata[\"features\"] = node_features\n",
        "    return g, node_features"
      ],
      "metadata": {
        "id": "R0Z5525UYVCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiagent PPO Policy for Reinforcement Learning (RL)"
      ],
      "metadata": {
        "id": "47xW4FDnYZkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAgentPolicy(nn.Module):\n",
        "    def __init__(self, action_dim, hidden_dim=128):\n",
        "        super(MultiAgentPolicy, self).__init__()\n",
        "\n",
        "        # CNN for local vision\n",
        "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, hidden_dim)\n",
        "\n",
        "        # GAT for communication\n",
        "        self.gat1 = GATConv(3, hidden_dim, num_heads=4)  # Input: (x, y, energy)\n",
        "        self.gat2 = GATConv(hidden_dim * 4, hidden_dim, num_heads=1)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion_fc = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "\n",
        "        # Output action layer\n",
        "        self.action_fc = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, unit_cnn_state, g, g_features):\n",
        "        # CNN processing\n",
        "        x_cnn = F.relu(self.conv1(unit_cnn_state))\n",
        "        x_cnn = F.relu(self.conv2(x_cnn))\n",
        "        x_cnn = x_cnn.view(x_cnn.shape[0], -1)  # Flatten\n",
        "        x_cnn = F.relu(self.fc1(x_cnn))\n",
        "\n",
        "        # GNN processing\n",
        "        x_gnn = F.relu(self.gat1(g, g_features))\n",
        "        x_gnn = self.gat2(g, x_gnn)\n",
        "        x_gnn = x_gnn.mean(dim=0)  # Aggregate graph info\n",
        "\n",
        "        # Fusion\n",
        "        x_fused = torch.cat([x_cnn, x_gnn], dim=-1)\n",
        "        x_fused = F.relu(self.fusion_fc(x_fused))\n",
        "\n",
        "        # Output action probabilities\n",
        "        return F.softmax(self.action_fc(x_fused), dim=-1)"
      ],
      "metadata": {
        "id": "a7vu-jUrYZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Action Selection"
      ],
      "metadata": {
        "id": "MQJhgYOZYgp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_multi_agent_actions(model, obs, team_id, env_cfg):\n",
        "    g, g_features = encode_team_graph(obs, team_id)\n",
        "    actions = {}\n",
        "\n",
        "    for unit_id, unit_pos in enumerate(obs[\"units\"][\"position\"][team_id]):\n",
        "        unit_cnn_state = encode_unit_view(obs, unit_pos, env_cfg).unsqueeze(0)  # Add batch dim\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_probs = model(unit_cnn_state, g, g_features)\n",
        "\n",
        "        action = torch.multinomial(action_probs, 1).item()  # Sample an action\n",
        "        actions[unit_id] = action\n",
        "\n",
        "    return actions"
      ],
      "metadata": {
        "id": "JLHSEw4eYg07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reward SHaping"
      ],
      "metadata": {
        "id": "zaB6QvS5Yor0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuxEnv(gym.Env):\n",
        "    def __init__(self, env_cfg, player: str):\n",
        "        super(LuxEnv, self).__init__()\n",
        "\n",
        "        self.env_cfg = env_cfg\n",
        "        self.player = player\n",
        "        self.opp_player = \"player_1\" if player == \"player_0\" else \"player_0\"\n",
        "\n",
        "        self.game = Game()\n",
        "        self.game_state = None\n",
        "\n",
        "        # Define action space and observation space\n",
        "        self.action_space = spaces.Discrete(5)  # e.g., move, harvest, build, etc.\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.env_cfg[\"map_width\"], self.env_cfg[\"map_height\"], 4), dtype=np.uint8)\n",
        "\n",
        "        # Initialize the game state\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to the initial state and return the initial observation.\"\"\"\n",
        "        self.game_state = self.game.reset(self.env_cfg)\n",
        "        return self.get_observation()\n",
        "\n",
        "    def get_observation(self):\n",
        "        \"\"\"Extract and return the game observation.\"\"\"\n",
        "        units = self.game_state.units[self.player]\n",
        "        unit_positions = np.array([unit.pos for unit in units])\n",
        "        unit_energies = np.array([unit.energy for unit in units])\n",
        "        relic_nodes = self.game_state.relic_nodes\n",
        "\n",
        "        map_grid = np.zeros((self.env_cfg[\"map_width\"], self.env_cfg[\"map_height\"], 4), dtype=np.uint8)\n",
        "\n",
        "        for pos in unit_positions:\n",
        "            map_grid[pos[0], pos[1], 0] = 1  # Mark unit positions\n",
        "\n",
        "        for relic_node in relic_nodes:\n",
        "            map_grid[relic_node[0], relic_node[1], 1] = 1  # Mark relic nodes\n",
        "\n",
        "        return map_grid\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take an action and return the next state, reward, done, and additional info.\"\"\"\n",
        "        actions = self.convert_action(action)\n",
        "        self.game_state = self.game.step(self.game_state, actions)\n",
        "\n",
        "        # Get observation after the step\n",
        "        observation = self.get_observation()\n",
        "\n",
        "        # Calculate the reward for the agent (based on resource collection, city survival, etc.)\n",
        "        reward = self.calculate_reward()\n",
        "\n",
        "        # Check if the episode is done\n",
        "        done = self.is_done()\n",
        "\n",
        "        # Info dictionary\n",
        "        info = {}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        \"\"\"Calculate the reward for the agent based on various milestones.\"\"\"\n",
        "        reward = 0\n",
        "\n",
        "        # Resource collection: Reward for each energy collected\n",
        "        for unit in self.game_state.units[self.player]:\n",
        "            reward += unit.energy  # Assuming reward is tied to energy collected\n",
        "\n",
        "        # City survival: Reward for keeping the city alive\n",
        "        if self.game_state.turns < self.env_cfg[\"max_turns\"]:\n",
        "            # City survival reward: +10 for each turn the city survives\n",
        "            reward += 10  # Or adjust based on city status\n",
        "\n",
        "        # Winning: Reward for winning the game\n",
        "        if self.game_state.is_game_over():\n",
        "            if self.game_state.winner == self.player:\n",
        "                reward += 100  # Large reward for winning\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def is_done(self):\n",
        "        \"\"\"Check if the episode is done.\"\"\"\n",
        "        # The episode is done if the game ends or if the city is destroyed\n",
        "        if self.game_state.turns >= self.env_cfg[\"max_turns\"]:\n",
        "            return True\n",
        "        return self.game_state.is_game_over()\n",
        "\n",
        "    def convert_action(self, action):\n",
        "        \"\"\"Convert RL action to Lux AI action.\"\"\"\n",
        "        actions = []\n",
        "\n",
        "        if action == 0:\n",
        "            actions = [(\"move\", unit_id, target_pos) for unit_id, target_pos in self.get_move_targets()]\n",
        "        elif action == 1:\n",
        "            actions = [(\"harvest\", unit_id) for unit_id in self.get_harvest_units()]\n",
        "        elif action == 2:\n",
        "            actions = [(\"build\", unit_id, building_type) for unit_id, building_type in self.get_building_targets()]\n",
        "        elif action == 3:\n",
        "            actions = [(\"explore\", unit_id) for unit_id in self.get_exploration_units()]\n",
        "        elif action == 4:\n",
        "            actions = [(\"collect_energy\", unit_id) for unit_id in self.get_energy_units()]\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render the current state of the game.\"\"\"\n",
        "        print(f\"Player points: {self.game_state.team_points[self.player]} - Opponent points: {self.game_state.team_points[self.opp_player]}\")\n",
        "        pass"
      ],
      "metadata": {
        "id": "YWQKVkesYo8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Opponent Difficulty"
      ],
      "metadata": {
        "id": "ErnmCF6WYsmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CurriculumTrainer:\n",
        "    def __init__(self, env_cfg, player):\n",
        "        self.env_cfg = env_cfg\n",
        "        self.player = player\n",
        "        self.current_stage = 1\n",
        "        self.opponent_difficulty = \"easy\"  # Initial opponent difficulty\n",
        "\n",
        "    def update_opponent_difficulty(self, agent_performance):\n",
        "        \"\"\"Adjust opponent difficulty based on agent performance.\"\"\"\n",
        "        if agent_performance >= 0.8:  # If agent is performing well, increase difficulty\n",
        "            self.current_stage += 1\n",
        "            if self.current_stage == 2:\n",
        "                self.opponent_difficulty = \"medium\"\n",
        "            elif self.current_stage == 3:\n",
        "                self.opponent_difficulty = \"hard\"\n",
        "            elif self.current_stage >= 4:\n",
        "                self.opponent_difficulty = \"expert\"\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the agent with the current stage and opponent difficulty.\"\"\"\n",
        "        if self.opponent_difficulty == \"easy\":\n",
        "            # Start training with easy opponent\n",
        "            opponent = EasyOpponent()\n",
        "        elif self.opponent_difficulty == \"medium\":\n",
        "            # Training with medium difficulty opponent\n",
        "            opponent = MediumOpponent()\n",
        "        elif self.opponent_difficulty == \"hard\":\n",
        "            # Training with hard opponent\n",
        "            opponent = HardOpponent()\n",
        "        else:\n",
        "            # Training with expert opponent\n",
        "            opponent = ExpertOpponent()\n",
        "\n",
        "        # Train agent against selected opponent\n",
        "        agent = Agent(self.player, self.env_cfg)\n",
        "        game = Game(agent, opponent)\n",
        "        game.run()\n",
        "\n",
        "        # Track agent's performance\n",
        "        agent_performance = agent.evaluate_performance()\n",
        "        self.update_opponent_difficulty(agent_performance)"
      ],
      "metadata": {
        "id": "pKvJwqDAYs17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust Opponent Based on Performance"
      ],
      "metadata": {
        "id": "VyuhlyoQYw8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_curriculum(self, agent):\n",
        "    \"\"\"Train the agent with progressively harder opponents.\"\"\"\n",
        "    # Training loop for several episodes\n",
        "    for episode in range(100):  # 100 training episodes as an example\n",
        "        game = Game(agent, opponent)\n",
        "        result = game.run()\n",
        "\n",
        "        # Track performance metrics\n",
        "        win_rate = result['win_rate']  # Track the agent's win rate\n",
        "        self.update_opponent_difficulty(win_rate)\n",
        "\n",
        "        # If agent is doing well, move to the next difficulty level\n",
        "        if self.current_stage > 1:\n",
        "            self.opponent_difficulty = \"medium\"\n",
        "        if self.current_stage > 2:\n",
        "            self.opponent_difficulty = \"hard\"\n",
        "        if self.current_stage > 3:\n",
        "            self.opponent_difficulty = \"expert\""
      ],
      "metadata": {
        "id": "IJc29eukYxN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Self-Play in Lux AI"
      ],
      "metadata": {
        "id": "pMgsD3PGY1dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfPlayTrainer:\n",
        "    def __init__(self, env_cfg, agent_cls, player, num_epochs=100):\n",
        "        self.env_cfg = env_cfg\n",
        "        self.agent_cls = agent_cls\n",
        "        self.player = player\n",
        "        self.num_epochs = num_epochs\n",
        "        self.previous_agents = []  # List to hold previous agent versions\n",
        "\n",
        "    def save_agent(self, agent):\n",
        "        \"\"\"Save the current agent to a list for future self-play.\"\"\"\n",
        "        self.previous_agents.append(agent)\n",
        "\n",
        "    def select_opponent(self, epoch):\n",
        "        \"\"\"Select an opponent based on the current epoch. The opponent will be a previous version of the agent.\"\"\"\n",
        "        # Opponent is selected from past agents\n",
        "        if epoch == 0:\n",
        "            return self.agent_cls(self.player, self.env_cfg)  # No opponent in first epoch, play against a simple random agent\n",
        "        return self.previous_agents[-1]  # Select the most recent version of the agent\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the agent using self-play, with progressively harder opponents.\"\"\"\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"Epoch {epoch + 1} Training Begins...\")\n",
        "\n",
        "            # Create the agent for this epoch\n",
        "            agent = self.agent_cls(self.player, self.env_cfg)\n",
        "\n",
        "            # Select opponent: play against previous agent or random strategy\n",
        "            opponent = self.select_opponent(epoch)\n",
        "\n",
        "            # Run the game (self-play against the selected opponent)\n",
        "            game = Game(agent, opponent)\n",
        "            result = game.run()  # Run a self-play game\n",
        "\n",
        "            # Evaluate the agent's performance\n",
        "            agent_performance = agent.evaluate_performance()\n",
        "\n",
        "            # Save the agent after this training epoch for future self-play\n",
        "            self.save_agent(agent)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1} completed. Agent Performance: {agent_performance}\")"
      ],
      "metadata": {
        "id": "L38HREQhY1t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Play Game Loop"
      ],
      "metadata": {
        "id": "EXsf8QKzY8sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Game:\n",
        "    def __init__(self, agent1, agent2):\n",
        "        self.agent1 = agent1\n",
        "        self.agent2 = agent2\n",
        "        self.env = LuxEnvironment()  # Lux AI environment\n",
        "        self.current_step = 0\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the game loop for a number of steps.\"\"\"\n",
        "        while not self.env.is_game_over():\n",
        "            obs1 = self.env.get_observation(self.agent1.player)\n",
        "            obs2 = self.env.get_observation(self.agent2.player)\n",
        "\n",
        "            # Get actions from both agents\n",
        "            actions1 = self.agent1.act(self.current_step, obs1)\n",
        "            actions2 = self.agent2.act(self.current_step, obs2)\n",
        "\n",
        "            # Step the environment forward\n",
        "            self.env.step(actions1, actions2)\n",
        "\n",
        "            self.current_step += 1\n",
        "\n",
        "        # At the end of the game, return the result\n",
        "        return self.env.get_game_result()"
      ],
      "metadata": {
        "id": "aEWv-yn-Y88M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric for Self-Play"
      ],
      "metadata": {
        "id": "uEh5dm83ZA03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(self):\n",
        "    \"\"\"Evaluate the agent’s performance based on custom metrics.\"\"\"\n",
        "    # Example of evaluating win rate\n",
        "    win_rate = self.calculate_win_rate()\n",
        "\n",
        "    # Other performance evaluations (e.g., resource efficiency)\n",
        "    resource_efficiency = self.calculate_resource_efficiency()\n",
        "\n",
        "    return {'win_rate': win_rate, 'resource_efficiency': resource_efficiency}\n",
        "\n",
        "def calculate_win_rate(self):\n",
        "    \"\"\"Calculate the agent's win rate.\"\"\"\n",
        "    # Placeholder: Assume agent plays 100 games and wins 80\n",
        "    return 0.8"
      ],
      "metadata": {
        "id": "GyXhYok3ZBBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid PPO + Q-Learning for Lux AI"
      ],
      "metadata": {
        "id": "arBrTz2cZGVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from stable_baselines3 import PPO\n",
        "from collections import deque\n",
        "\n",
        "class HybridAgent:\n",
        "    def __init__(self, player: str, env_cfg, alpha=0.001, gamma=0.99, epsilon=0.1):\n",
        "        self.player = player\n",
        "        self.env_cfg = env_cfg\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Initialize PPO agent\n",
        "        self.ppo_agent = PPO(\"MlpPolicy\", env_cfg, verbose=0)\n",
        "\n",
        "        # Initialize Q-learning\n",
        "        self.q_table = np.zeros((env_cfg[\"state_space_size\"], env_cfg[\"action_space_size\"]))\n",
        "\n",
        "    def select_action_ppo(self, obs):\n",
        "        \"\"\"Select action using PPO (on-policy)\"\"\"\n",
        "        action, _ = self.ppo_agent.predict(obs)\n",
        "        return action\n",
        "\n",
        "    def select_action_qlearning(self, state):\n",
        "        \"\"\"Select action using Q-learning (off-policy)\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Exploration: Random action\n",
        "            return np.random.choice(self.env_cfg[\"action_space_size\"])\n",
        "        else:\n",
        "            # Exploitation: Choose the best action based on Q-table\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        \"\"\"Update the Q-table using Q-learning update rule\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.alpha * td_error\n",
        "\n",
        "    def train(self, env, steps=1000):\n",
        "        for step in range(steps):\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                # Decide on action using a combination of PPO and Q-learning\n",
        "                if step % 2 == 0:  # Alternate between PPO and Q-learning\n",
        "                    action = self.select_action_ppo(obs)\n",
        "                else:\n",
        "                    state = self.get_state_from_obs(obs)  # Convert observation to state\n",
        "                    action = self.select_action_qlearning(state)\n",
        "\n",
        "                next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "                # Update Q-table\n",
        "                if step % 2 == 1:  # Q-learning update after every second step\n",
        "                    next_state = self.get_state_from_obs(next_obs)\n",
        "                    self.update_q_table(state, action, reward, next_state)\n",
        "\n",
        "                # Save the experience for PPO training\n",
        "                if step % 2 == 0:\n",
        "                    self.ppo_agent.learn(total_timesteps=1)  # Simulate learning\n",
        "\n",
        "                obs = next_obs\n",
        "\n",
        "    def get_state_from_obs(self, obs):\n",
        "        \"\"\"Convert the environment observation to a state for Q-learning.\"\"\"\n",
        "        # Example transformation from obs to state (this would depend on the environment)\n",
        "        return np.digitize(obs, self.env_cfg[\"state_bins\"])"
      ],
      "metadata": {
        "id": "wiwvTk4LZGk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization in PyTorch"
      ],
      "metadata": {
        "id": "JtEAA6iPZMFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = torch.load(\"lux_ai_model.pth\")\n",
        "\n",
        "# Apply dynamic quantization (applies to weights only, keeping activations as float)\n",
        "quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Save the quantized model\n",
        "torch.save(quantized_model, \"lux_ai_quantized_model.pth\")"
      ],
      "metadata": {
        "id": "BdLK9ov3ZMVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Distillation in PyTorch"
      ],
      "metadata": {
        "id": "uVIs4_zNZRlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Add the Lux AI environment to your path\n",
        "sys.path.append(os.path.abspath('Lux-Design-S3/src'))\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import PPO\n",
        "from luxai_s3.lux_env import LuxEnv\n",
        "\n",
        "# 1. Define the Teacher Model Architecture\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        # Define your model layers here, e.g., for a simple MLP:\n",
        "        self.fc1 = nn.Linear(observation_space.shape[0], 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation for the output layer (usually)\n",
        "        return x\n",
        "\n",
        "# 2. Create the Lux AI Environment\n",
        "env = LuxEnv(\n",
        "    configs={\n",
        "        \"seed\": 562124210,\n",
        "        \"loglevel\": 2,\n",
        "        \"annotations\": True,\n",
        "        \"width\": 12,\n",
        "        \"height\": 12,\n",
        "        \"max_episode_length\": 1000,  # number of steps in the environment\n",
        "    },\n",
        "    learning_agent=\"player_0\",\n",
        "    opponent_agent=\"player_1\",\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "# 3. Instantiate the Teacher Model\n",
        "teacher_model = TeacherModel(env.observation_space, env.action_space)\n",
        "\n",
        "# 4. Set up the Optimizer\n",
        "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=0.001)\n",
        "\n",
        "# 5. Training Loop (Example - Replace with your training logic)\n",
        "for episode in range(num_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Get the teacher's action\n",
        "        action = teacher_model(torch.tensor(obs, dtype=torch.float32))\n",
        "        action = torch.argmax(action).item()  # Choose the action with highest probability\n",
        "\n",
        "        # Take the action in the environment\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "        # Calculate loss and update the teacher model\n",
        "        # ... (your loss calculation and optimization logic here) ...\n",
        "\n",
        "        obs = next_obs\n",
        "\n",
        "# 6. Save the Trained Teacher Model\n",
        "torch.save(teacher_model.state_dict(), \"teacher_model.pth\")"
      ],
      "metadata": {
        "id": "q_lx_sNGZRzC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}